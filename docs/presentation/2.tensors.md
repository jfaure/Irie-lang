# Array oriented calculus of constructions
* Everything that can be treated as an array, is.

## Arity and Tensors
```haskell
[1 2 3] == MkArray 1 2 3 -- lists =~ var-arity
type Arrow (t:Type) = Stack (->) t -- Arrows are stacks of at least one type
```

## Memory
### Stack Frame
Functional programming involves collecting arguments with which to tail-call functions. Thus programs form acyclic graphs and data flow can be seen clearly. What follows is a description, from best to worst, of the properties of the ideal memory management scheme

1. Stack memory is our dream. We pass down pointers to stack memory for functions returning data. This is possible when we can identify both the last use and the size of a data. Unfortunately; constructors, case decons and switchcases can obscure lifetimes (eg. by hiding subdata within other data), while recursion, switchcases and IO can obscure sizes.
2. Linear typing + heap allocation is our backup, and applies when a function can promise to use all of it's arguments exactly once, and the return data + all subdata are also linear.
3. Dynamic reference counted heap allocations are a catch-all.

Thoughts
* Arrays == Var args
* structural linearity ( broken by switch )
* types of case: quantitative product decon (like fn args) + special case for arrays, where elem extraction is complicated
* If functions return data conditionally (switch on contents of the data), stack memory passdown is no longer ideal
* mixture of memory techniques ?
* Let's may cause sharing of data (specially complex is shared subdata)
* Rec's are disasterous
* choice is in the type - llvm representation may be different (ref counts / inline subdata..) and conversions nontrivial

### Heapless
* small data (eg. an int) is simply copied.
* the cost (space and time) of using the heap is massive (overhead of memory, in the form of chunk headers and fragementation, and time involved in sorting and coalescing chunks)
* huge allocations (> MMAP_THRESHOLD, 128KB by default) are handled by mmap (MAP_ANONYMOUS)

Any function that returns data (incl. transitively) takes a pointer to stack memory: (maybe > returned value !). ie. pass down worst case space requirements: the largest sumtype alt and (if returned conditionally) the sum of each branch

* a simple approach is to inline all subdata - but this forces copies of subdata that outlives it's parent. The subdata can alternatively be a pointer.

### Recursive Data, The Heap and Linear types
recursive calls to data constructors tend to be synonymous with impredicable memory requirements. Unless inlineable onto the stack frame of it's last use, we must use the heap; which means both that we must later explicitly deallocate, and that we cannot use our traditional technique of passing down stack memory. Thus recursive data calls for different memory management, one that is hinted at by considering a related problem: Functor like operations often wish to update data inplace - which is safe iff the data is used linearly. So, enter linear types, or "deallocate after use", The idea being to make readers responsible for freeing memory.
1. Recursive data (and huge arrays as a special case) are allocated on the heap; they need to be freed at some point
2. Certain functions (esp. fmap) wish to write to data/resource in place - this means the resource can't be referenced elsewhere.

In this model, parent data is responsible for all subdata. This is all rather straightforwards, until one decides one wants to call a linear function on a non-linear resource. In the case of effectful functions, non-linear use is flat-out disallowed.

### Pseudo-linear types
Strictly speaking, trying to use a linear variable non-linearly is at odds with the idea of a linear variable, with such pseudo-linearity being essentially a hack to try to avoid unnecessary copying. Thus pseudo-linear variables are often reference-counted. A compelling example is the desire to inplace a pseudo-linear array; one can wrap it in an IntMap of overloaded indices , and perhaps outlive the other references and directly inplace the underlying array.

### Summary
There are several complications when handling memory
1. subdata outlives parent
2. unknown size (recursive data).
3. unknown returned (via case)
4. inplace updates.
5. 100% unknown (polymorphic function pointer)

## Effects
Effects are extremely well suited to being described with linear logic. In fact by forcing the use of linear functions, there remains only one question: how to combine effects.
* Some effects need not be sequenced (eg. reads ; only writes should force sequencing)

```haskell
-- normal type for main
type IO = In & Out -- IO is a polytype of effects
main : [String] -> Byte % IO
main = print (filter (\match '.' , _) readDir pwd)
```
What is the type of an Effect ? Pretty much a product polytype (&). Ultimately stateful effects need to become monads, after which 2 questions arise:
1. How to insert the effect combinators `<$> <*> >>=`
2. How to derive effect combinators: `(>>=)` is distributive over composition for many monads, but not all (eg. `Maybe`)
